{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# Building a RAG chatbot with LangChain, Hugging Face, Amazon SageMaker and Amazon OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91613d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/rich/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rich/Desktop/gen-ai-hackton/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from transformers import AutoConfig\n",
    "from typing import Dict\n",
    "\n",
    "from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb259e8",
   "metadata": {},
   "source": [
    "## Deploy our LLM on a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd31539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role = sagemaker.get_execution_role()\n",
    "role = 'arn:aws:iam::070576557102:role/service-role/AmazonSageMaker-ExecutionRole-20240512T164029'\n",
    "\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'taide/Llama3-TAIDE-LX-8B-Chat-Alpha1-4bit',\n",
    "\t'SM_NUM_GPUS': '1'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role \n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "    wait=False,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52098837",
   "metadata": {},
   "source": [
    "## Configure the LangChain input and output handlers for our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de541096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            # Mistral prompt, see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "            {\"inputs\": f\"<s>[INST] {prompt} [/INST]\", \"parameters\": {**model_kwargs}}\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        splits = response_json[0][\"generated_text\"].split(\"[/INST] \")\n",
    "        return splits[1]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1673ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker') # needed later to check that endpoint is up\n",
    "smrt_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=smrt_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca9201",
   "metadata": {},
   "source": [
    "## Load the Reuters news dataset from the Hugging Face hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "## Configure our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa50471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import logging\n",
    "# import boto3\n",
    "# from botocore.exceptions import ClientError\n",
    "# from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# def generate_text_embeddings(model_id, docs):\n",
    "#     \"\"\"\n",
    "#     Generate text embeddings by using the Cohere Embed model.\n",
    "#     Args:\n",
    "#         model_id (str): The model ID to use.\n",
    "#         docs (list): A list of Document objects to generate embeddings for.\n",
    "#     Returns:\n",
    "#         dict: The response from the model.\n",
    "#     \"\"\"\n",
    "#     logger.info(\"Generating text embeddings with the Cohere Embed model %s\", model_id)\n",
    "#     accept = '*/*'\n",
    "#     content_type = 'application/json'\n",
    "#     bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "#     texts = [doc.page_content for doc in docs]  \n",
    "#     body = json.dumps({\"texts\": texts, \"input_type\": \"search_document\"})\n",
    "#     response = bedrock.invoke_model(\n",
    "#         body=body,\n",
    "#         modelId=model_id,\n",
    "#         accept=accept,\n",
    "#         contentType=content_type\n",
    "#     )\n",
    "#     logger.info(\"Successfully generated text embeddings with Cohere model %s\", model_id)\n",
    "#     return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2568df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "# model_id = 'cohere.embed-english-v3'\n",
    "\n",
    "# # Load documents from PDF files\n",
    "# loader = PyPDFDirectoryLoader(\"/Users/rich/Desktop/gen-ai-hackton/data\")\n",
    "# docs = loader.load()\n",
    "\n",
    "# try:\n",
    "#     response = generate_text_embeddings(model_id=model_id, docs=docs)\n",
    "#     response_body = json.loads(response.get('body').read())\n",
    "#     print(f\"ID: {response_body.get('id')}\")\n",
    "#     print(f\"Response type: {response_body.get('response_type')}\")\n",
    "#     print(\"Embeddings\")\n",
    "#     for i, embedding in enumerate(response_body.get('embeddings')):\n",
    "#         print(f\"\\tEmbedding {i}\")\n",
    "#         print(*embedding)\n",
    "#     print(\"Texts\")\n",
    "#     for i, text in enumerate(response_body.get('texts')):\n",
    "#         print(f\"\\tText {i}: {text}\")\n",
    "# except ClientError as err:\n",
    "#     message = err.response[\"Error\"][\"Message\"]\n",
    "#     logger.error(\"A client error occurred: %s\", message)\n",
    "#     print(\"A client error occurred: \" + format(message))\n",
    "# else:\n",
    "#     print(f\"Finished generating text embeddings with Cohere model {model_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6131c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def generate_text_embeddings(model_id, docs):\n",
    "    \"\"\"Generate text embeddings by using the Cohere Embed model.\"\"\"\n",
    "    logger.info(\"Generating text embeddings with the Cohere Embed model %s\", model_id)\n",
    "    accept = '*/*'\n",
    "    content_type = 'application/json'\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    \n",
    "    # Split texts into batches of 128\n",
    "    batched_texts = [texts[i:i+128] for i in range(0, len(texts), 128)]\n",
    "    \n",
    "    all_embeddings = []\n",
    "    for batch in batched_texts:\n",
    "        body = json.dumps({\"texts\": batch, \"input_type\": \"search_document\"})\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        all_embeddings.extend(response_body.get('embeddings'))\n",
    "    \n",
    "    logger.info(\"Successfully generated text embeddings with Cohere model %s\", model_id)\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c19d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "model_id = 'cohere.embed-english-v3'\n",
    "\n",
    "# Load a single PDF file\n",
    "loader = PyPDFLoader(\"/Users/rich/Desktop/gen-ai-hackton/data/6nrhqkvp47vz7hnrzyl7oy7pjdmv.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf43ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "host = 'vpc-mydomain-zzliggvd46i3slgldu6mk55gmu.ap-northeast-1.es.amazonaws.com'\n",
    "index_name = 'index'\n",
    "region = 'ap-northeast-1'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, \"aoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c216153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:__main__:Generating text embeddings with the Cohere Embed model cohere.embed-english-v3\n",
      "INFO:__main__:Successfully generated text embeddings with Cohere model cohere.embed-english-v3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenSearchService' object has no attribute 'bulk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             documents\u001b[38;5;241m.\u001b[39mappend(document)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# 使用 boto3 客戶端將文檔索引到 OpenSearch\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk\u001b[49m(\n\u001b[1;32m     29\u001b[0m             body\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[1;32m     30\u001b[0m             index_name\u001b[38;5;241m=\u001b[39mindex_name,\n\u001b[1;32m     31\u001b[0m             domain_name\u001b[38;5;241m=\u001b[39mdomain_name,\n\u001b[1;32m     32\u001b[0m         )\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated vector search index for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(doc_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/gen-ai-hackton/.venv/lib/python3.11/site-packages/botocore/client.py:918\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 918\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenSearchService' object has no attribute 'bulk'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# 創建 OpenSearch 客戶端\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    \"opensearch\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 生成文檔嵌入向量\n",
    "    embeddings = generate_text_embeddings(model_id=model_id, docs=docs)\n",
    "\n",
    "    # 按批次處理文檔\n",
    "    docs_100 = [docs[x:x+100] for x in range(0, len(docs), 100)]\n",
    "    for doc_batch in docs_100:\n",
    "        # 構建要索引的文檔\n",
    "        documents = []\n",
    "        for doc, embedding in zip(doc_batch, embeddings):\n",
    "            document = {\n",
    "                \"id\": doc.metadata.get(\"id\", \"\"),\n",
    "                \"text\": doc.page_content,\n",
    "                \"embedding\": embedding,\n",
    "                # 添加其他元數據字段...\n",
    "            }\n",
    "            documents.append(document)\n",
    "\n",
    "        # 使用 boto3 客戶端將文檔索引到 OpenSearch\n",
    "        response = client.bulk(\n",
    "            body=documents,\n",
    "            index_name=index_name,\n",
    "            domain_name=domain_name,\n",
    "        )\n",
    "\n",
    "        print(f\"Created vector search index for {len(doc_batch)} documents.\")\n",
    "except ClientError as err:\n",
    "    message = err.response[\"Error\"][\"Message\"]\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(\"A client error occurred: \" + format(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec6feb",
   "metadata": {},
   "source": [
    "## Define credentials for Amazon OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df511e4",
   "metadata": {},
   "source": [
    "## Embed and index chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe581fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_100 = [docs[x:x+100] for x in range(0, len(docs), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab34d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for docs in docs_100:\n",
    "    oss = OpenSearchVectorSearch.from_documents(\n",
    "        docs,\n",
    "        embeddings=model_id,\n",
    "        opensearch_url=f'https://{host}:443',\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        index_name=index_name,\n",
    "        timeout=60,\n",
    "    )\n",
    "    print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "## Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = oss.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1db22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "As a helpful news agent, please answer the question using only the context below.\n",
    "If you don't know, say you don't know.\n",
    "Cite the title of the articles you used to build your answer.\n",
    "\n",
    "question: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that our LLM has been deployed\n",
    "\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "## Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64223366",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the worst storms in recent news?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f083366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
